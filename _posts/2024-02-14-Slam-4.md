---
title: "[SLAM] Cameras and Images"
date: 2024-02-14 1:00:00 +0900
categories: [3D]
tags: [SLAM]
---


# Introduction
- 이 챕터에서는 observation equation에 속하는 내용으로, 로봇이 어떻게 외부 세계를 보는지를 다룬다.
- 카메라 기반 visual SLAM에서는 주로 image projection이 된다.
- 우리는 real life에서 다양한 사진들을 본다. 색상과 밝기 정보를 담은 사진들은 컴퓨터에 수백만 개의 픽셀로 표현된다.
- 3차원 물체에 반사된 빛은 카메라의 optical center를 통과하고 카메라의 imaging plane에 투영되게 된다. 다음으로 카메라의 빛 센서가 빛을 받으면, measurement를 생성하고 픽셀을 만들며, 이게 우리가 보는 사진이 된다. 이 과정이 수학적 수식으로 표현될 수 있을까?
- 이 챕터에서는 camera model에 대해 다루며, 이때의 projection relationship과 internal parameter에 대해 설명한다.




# Pinhole Carmera Models
- 3D point를 2D image plane에 projection할 때 사용하는 geometric model에는 다양한 방식이 있다. 가장 간단한 방식으로는 pinhole model이 있는데, 우리는 이 **pinhole model**로 projection을 시작해 볼 것이다.
- 또한 카메라 렌즈 때문에 projection 동안에는 distortion이 발생한다. 그래서 전체 projection 과정을 묘사하기 위해 pinhole model과 함께 distortion model을 사용할 것이다.



## Pinhole Camera Geometry
- 다들 고등학교 과학시간에 초로 projection하는 실험을 해봤을 것이다. 초가 어두운 상자 앞에 있고, 초의 불빛은 작은 구멍을 통해서 어두운 상자 뒤쪽 평면에 project된다. 그러면 수직으로 뒤집어진 초의 image가 이 평면에 맺히게 된다. 이 과정에서, 작은 구멍은 3차원 세계의 초를 2차원 imaging plane에 투영하게 된다.
- 마찬가지로 우리도 간단한 모델을 통해 camera의 imaging process를 표현할 수 있다. Figure 4-1을 보자.
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/3a6fa8ca-658c-4ac9-8aea-751e6396d81d)


- camera coordinate system
    - $$O-x-y-z$$
    - z축: 카메라 앞쪽, x축: 카메라 오른쪽, y: 카메라 아래쪽
    - $$O$$: 카메라 optical center (pinhole 모델의 hole)
    - $$P$$: 3D point
        - imaging plane의 $$O^\prime-x^\prime-y^\prime$$
    - $$P^\prime$$: image point
        - $$P$$의 coordinate이 $$[X,Y,Z]^T$$가 되고, $$P^\prime$$이 $$[X^\prime,Y^\prime,Z^\prime]^T$$이 된다고 하자.
        - imaging plane에서 camera plane까지의 focal length는 $$f$$가 된다.
        - 그러면 삼각형의 닮음에 의해서(Figure 4-1의 오른쪽 그림 참고) 다음과 같은 공식이 나온다.
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/8042f6ab-f4d4-43b2-aad0-6e4aef2fd195)
        - 음수 부호는 이미지가 뒤집혀 있음을 의미
    - 현대 카메라로 찍은 카메라는 뒤집혀있지 않으므로, Figure 4-2처럼 image plane을 symmetric하게 표현할 수 있다.
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/0de94ae5-c6fe-4291-80de-354c34909a80)
    - 그럼 수식 4.1에서 음수 부호를 제거할 수 있다.
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/04adc4ad-f999-4279-8d18-8225638a290e)
- image coordinates    
    - 여기서 $$X^\prime, Y^\prime$$을 좌변으로 보내면 다음과 같다.
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/bda4c387-f238-416a-8fc4-6e4183af9c04)
    - 수식 4.3은 point $$P$$와 그 image 간의 공간적 관계임. 단위는 meter
        - 예를 들어, focal length가 0.2미터이면, $$X\prime$$이 0.14미터


- pixel coordinates
    - 카메라가 광학 정보를 인식하여 image pixel로 옮길 때, imaging plane에 고정된 pixel plane $$o-u-v$$로 옮김.
    - $$P\prime$$의 좌표는 $$[u, v]^T$$
        - origin $$o\prime$$은 이미지의 왼쪽 위 코너, $$u$$축은 $$x$$축과 평행한 축, $$v$$축은 $$y$$축과 평행한 축
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/35b5eb42-406f-40f0-8907-c30920895c61)
        - $$\alpha$$: pixels/meter(u축 scale), $$\beta$$: pixels/meter(v축 scale)
        - $$[c_x,c_y]^T$$: 옮겨진 origin
    - 수식 4.3을 대입
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/04686861-851d-4f22-92ac-f1d3ed0d2cd2)
        - $$\alpha f$$를 $$f_x$$, $$\beta f$$를 $$f_y$$로 표현
    - 단위는 pixel


- matrix로 표현
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/09071df7-6d33-4b66-b574-b3cf13c1ab93)
    - homogeneous coordinate $$K$$와 non-homogeneous coordinate $$P$$
    - $$Z$$를 왼쪽에 놓으면
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/4a36d1b2-138c-44aa-bc3c-7a1ac87670ca)
        - internal parameter
            - $$K$$는 카메라의 inner parameter matrix로 fixed된 값임
        - external parameter
            - 수식 4.6에서는 camera 좌표계를 사용했지만, 카메라가 계속 움직이기 때문에 원래 P의 좌표계는 world 좌표계임 (기호 $$P_w$$)
            - $$P_w$$가 camera pose에 따라 camera 좌표계로 변환됨
            - camera pose는 rotation matrix $$R$$과 translation vector $$t$$로 표현
        - relationship of world coordinates to pixel coordinates
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/756c57c0-857b-48ca-8dda-07cbf2a30a09)
            - 마지막 식에서는 homogeneous coordinates를 non-homogeneous coordinates로 변환함
            - $$TP_w$$에서 homogeneous coordinates를 사용하고, non-homogeneous coordinates로 변환하고, 그걸 $$K$$로 곱함


- 정리하면 다음과 같다.
![KakaoTalk_20240214_161545375](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/b58399bd-2ba0-44e1-ac3e-3b0f0668305c)


- normalized coordinates
    - projection 과정을 다른 관점으로 바라볼 수 있음
    - 수식 4.8을 보면 world coordinate point를 camera coordinate으로 변환하고 마지막 차원을 제거해서 사용할 수 있다는 걸 보여준다.
![KakaoTalk_20240214_165306882](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/be14d18c-2eaf-4275-84d9-f92c018e19a8)
    - 즉, imaging plane의 깊이 정보는 삭제되고 이건 마지막 차원을 normalization하는 것과 같음
    - 이러면 camera normalized plane 상에 point $$ P $$의 projection를 얻을 수 있음
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/c9b0d9f4-dfb3-4428-bf44-de3795d11aa0)
    - normalized coordinates는 $$z=1$$인 plane의 한 점으로도 볼 수 있음
    - pixel coordinates는 normalized plane 위의 point를 quantative measurement한 결과로도 볼 수 있음


## Distortion
- 더 넓은 FoV(Field-of-View)를 얻기 위해서, 우리는 카메라 앞에 렌즈를 더함. 렌즈를 더하면 imaging 과정에서 빛이 들어올 때 왜곡이 발생함.
    - (1) 렌즈의 모양 때문에 빛의 진행이 왜곡 (2) lens와 imaging plane이 완전히 평행하지 않아서 projection 시 position이 왜곡

- Pinhole camera vs Real photos
    - pinhole camera에서는 빛이 일직선으로 진행하여 pixel plane에 정확한 상이 맺힘
    - real environment에서는 빛에 curve가 생김
    - 이미지의 edge에 가까워질 수록 이 현상은 심해짐

- Distortion의 종류
    1. 렌즈의 모양 때문에 발생하는 왜곡 (radial distortion)
        - lens는 일반적으로 center-symmetrical하므로, 아래처럼 symmetrical한 distortion이 발생함
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/cf6e6328-de6c-4203-8d8e-bb302df7f957)
        - barrel distortion
            - 렌즈의 중심에서 멀어질수록 픽셀의 범위가 작아짐
        - cushion distortion
            - 렌즈의 중심에서 멀어질수록 픽셀의 범위가 커짐
        - 두 distortion에서 모두 이미지의 중심과 optical axis를 관통하는 선은 유지됨
    2. lens와 imaging surface가 완전히 평행하지 않아서 발생하는 왜곡
        - tangential distortion
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/1fb7cabd-6d9c-42d5-9a99-5d7b3575e7db)

- 더 깊은 이해
    - normalized plane의 한 점 $$p$$가 있을 때, $$[x,y]^T$$ 혹은 극좌표에서는 $$[r,\theta]^T$$로 표현됨
        - $$r$$은 point $$p$$와 원래 좌표계의 거리
        - $$\theta$$는 수평 축으로의 각도
    - radial distortion은 좌표점이 length를 따라 왜곡되는 것
    - tangential distortion은 좌표점이 horizontal angle을 따라 왜곡되는 것

- normalized coordinates after distortion
    - normalized coordinates $$(x, y)$$를 통해 distortion 이후의 $$[x_{distorted}, y_{distorted}]^T$$를 구할 수 있음
    - $$k$$는 radial distortion coefficient, $$p$$는 tangential distortion coefficient
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/b0e4bebe-2be2-4b7a-9e25-02ff9791248b)


# Stereo Cameras
- single camera는 depth 정보를 제공하지 못함
- binocular camera나 RGB-D camera로 $$P$$의 depth가 결정될 때, 그 공간적 위치를 알 수 있음
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/8e74f33d-ceb0-481e-b40f-772f6cac43ec)
    - normalized plane 상에 점 $$P$$가 있을 때, 있을 것으로 예측되는 $$Real P$$의 위치를 보여줌
- pixel distance(depth)를 재는 방법
    - 사람 눈은 왼쪽과 오른족 눈에서 보이는 scene의 차이로 물체의 거리를 측정함
    - binocular camera도 똑같다.
        - 왼쪽과 오른쪽 이미지를 동시에 얻고 이미지간 차이를 구하면, 각 픽셀의 depth가 측정됨


- binocular camera
    - left-eye camera와 right-eye camera로 구성
    - 두 카메라 모두 pinhole camera라고 가정하며, center가 모두 같은 x축 상에 있음
    - 두 center간 차이를 baseline이라 부름
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/b643d5e0-a8b2-4b69-bb5c-ee58aecff353)
    - 3D point $$P$$
        - left-eye와 right-eye에 project되면 $$P_L, P_R$$이 됨
        - 둘은 x축을 따라 이동했기 때문에, P의 image도 x축에서만 다르게 표현됨
        - left pixel coordinate $$u_L$$과 right pixel coordinate $$u_R$$
        - $$\triangle P P_L P_R$$과 $$\triangle P O_L O_R$$의 닮음에 의해, 다음과 같은 수식이 나옴
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/85647408-9cb8-4854-9f22-32dc8c23b41f)
        - 식을 다시 정리하면 다음처럼 됨
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/d66b87ec-d07b-48a8-8c6a-dd4a95af5031)
        - $$d$$는 왼쪽과 오른쪽의 horizontal 좌표 사이의 차이이며, disparity 혹은 parallax라고 불림
    - parallax는 distance와 반비례하는 관계
        - parallax가 클수록, distance가 가까워짐
        - parallax는 최소한 1픽셀 이상이어야 하므로, binocular depth의 이론적 최댓값은 $$fb$$임
    - 연산량 때문에, binocular depth estimation은 GPU나 FPGA가 필요함


# RGB-D Cameras
- binocular camera에 비해 RGB-D camera는 더 active하게 depth를 측정한다.
- RGB-D camera 종류
    1. structured infrared light으로 pixel distance를 측정하는 방식
        - 카메라가 돌아오는 structured light pattern으로 물체와 카메라간 거리를 측정함
    2. time-of-flight (ToF)로 pixel distance를 측정하는 방식
        - 카메라가 빛을 발사해 발사 시간을 통해 distance를 측정하는 방식
- Output
    - depth 측정 이후, RGB-D 카메라는 depth와 color map pixel 사이를 연결함
    - pixel 단위로 호응되는 color image와 depth image를 출력
- 문제점
    - infrared light을 사용하는 카메라는 일광이나 다른 센서의 방해에 취약해서 외부 환경에서 사용 불가
    - 투명한 물체는 빛을 반사하지 못해서 측정 불가

# Images
- 컴퓨터에서의 image
    - 컴퓨터는 실수를 표현할 수 없기 때문에, 특정 범위 내에서 이미지를 표현함
    - $$x, y$$는 0부터 $$w-1, h-1$$까지의 정수임
![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/4abbe4cc-9e0d-44cf-a419-97624b6d47d9)