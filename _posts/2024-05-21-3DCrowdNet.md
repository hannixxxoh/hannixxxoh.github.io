---
title: "3DCrowdNet"
date: 2024-05-21 1:00:00 +0900
categories: [Paper Review]
tags: [human, 3D, SMPL]
---

## 논문
https://arxiv.org/pdf/2104.07300

본 논문은 monocular single image에서 3D human mesh를 estimation하는 논문입니다.

## Human mesh recovery
기존의 human mesh recovery 분야에서는 mixed-batch training을 활용합니다. Mixed-batch training에서 일부는 MoCap 3D dataset으로 구성하고, 일부는 in-the-wild 2D dataset으로 구성합니다. 2D 데이터셋을 supervision으로 활용하려면 우선 mesh를 예측하여 3D joint를 regression한 뒤, image plane으로 투영합니다. 그 다음 투영된 2D joint와 ground truth간의 거리를 계산합니다. 이런 식으로 3D와 2D 데이터셋을 섞어서 쓰면 accuracy와 generalization 향상에 도움이 됩니다. 3D annotation으로부터 정확성을 높일 수 있고, 2D 이미지로부터 다양한 이해력을 높일 수 있기 때문입니다.

또한 global feature vector를 사용하는 model-based approach를 활용합니다. deep-CNN으로 feature vector를 추출한 뒤, 그로부터 human model parameter(SMPL)을 추출하는 식입니다. 우선, 기존의 human detector를 사용해서 target person의 bounding box를 찾고, bounding box를 crop합니다. 그 다음 crop된 이미지를 deep-CNN으로 추출한 뒤 global average pooling을 수행해서 global feature vector를 얻습니다. 이 global feature vector는 MLP 기반의 regressor를 통과하고 mesh parameter를 얻습니다. 

## 기존 연구의 문제점
기존의 3D human mesh estimation 연구들은 benchmark dataset에 대해서는 좋은 성능을 보이고 있지만, test시 crowded scene에서는 결과가 좋지 않습니다. 이유는 두 가지가 있습니다.

![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/712974bf-ac06-4809-91ca-e9411d9bf10c)

### The domain gap
MoCap dataset과 in-the-wild crowded scene의 testing data 간의 domain gap입니다. Mocap dataset은 motion capture data이기 때문에 crowded scene이 부족하지만, 실제 in-the-wild crowded scene에서는 inter-person occusion, interacting body poses, indistinguishable cloth appearances 등의 문제가 발생합니다. 이는 deep-CNN이 target person의 적절한 image feature를 추출하는 데 방해가 됩니다. 따라서 MoCap dataset에서는 잘 작동하던 human mesh estimation 모델들도 human crowd에서는 성능이 떨어지게 됩니다. 이를 해결하기 위하여 mixed-batch training을 적용할 수 있지만, domain gap을 완전히 극복하기에는 부족합니다.

### Global average pooling
deep-CNN feature을 global average pooling할 때 spatial information이 손실되는 문제입니다. 이 spatial information은 target person과 다른 사람들을 구분하는 feature를 포함하고 있습니다. In-the-wild crowded scene은 겹쳐 있는 사람이나 부정확한 human bounding box를 포함하고 있는 경우가 많은데, 그러면 target bounding box에 non-target people도 포함되게 됩니다. deep-CNN feature는 이 non-target people의 feature도 처리하게 되고, GAP를 거치면 target person과 다른 사람들의 feature가 구분되지 않게 됩니다. 이로 인해 regressor에 혼란을 야기하고 target person의 정확한 3D pose를 추정하기 어렵게 됩니다.

## 3DCrowdNet
따라서 본 논문에서는 in-the-wild crowded scene에서 3D human mesh를 추정하는 방법론을 제안합니다. 3DCrowdNet은 위에서 제기한 두 가지 문제점에 대한 해결방안을 제시합니다.

첫번째로, 3DCrowdNet은 **off-the-shelf 2D pose estimator**를 사용하여 domain gap을 해결합니다. 2D pose estimator는 in-the-wild dataset으로만 학습되었기 때문에, human crowd images에 대한 이해력이 높고 inference시 crowded scene에 대한 domain gap을 겪지 않습니다. 본 논문에서는 2D pose estimator를 crowded scene-robust image feature로 활용하였습니다. 저자들은 이 2D pose estimator가 target person에 대한 강력한 근거를 제공하고 crowded scene에서도 모델이 target person의 feature에 집중할 수 있도록 했다고 주장합니다.

두번째로, **joint-based regressor**를 통해 GAP를 대체합니다. joint 기반의 regressor는 우선 joint의 공간적 위치를 예측합니다. 다음으로, joint의 공간적 위치에 기반하여 deep-CNN feature map으로부터 image feature를 추출합니다. joint의 공간적 위치에 기반하기 때문에 target person과 다른 사람의 feature는 구분됩니다. Human model parameter는 sample image features로부터 regression을 통해 예측됩니다.

### Architecture
3DCrowdNet은 feature extractor와 joint-based regressor로 구성되어 있습니다.

![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/0d5e6baf-02e0-48b6-b22e-d3569c94e04a)


#### Feature extractor

Feature extractor는 **2D pose**와 **cropped image**를 input으로 사용합니다. 사람이 있는 영역의 bounding box를 잡고, 이를 crop한 뒤 256X256으로 resize합니다. 이때 crop한 이미지와, crop한 이미지에서 사람의 2D pose를 estimate한 joint map을 모델의 입력값으로 활용합니다.

**2D pose**는 bottom-up off-the-shelf estimator를 사용하여 2D joint coordinates $$\mathbf{P}^{2D} \in \mathbb{R}^{J \times 2}$$로 예측합니다. 이때 $$J$$는 human joint의 개수를 의미하며 2D pose estimator마다 바뀔 수 있습니다. 3DCrowdNet은 2D pose $$\mathbf{P}^{2D}$$를 heatmap $$\mathbf{H}^{2D} \in \mathbb{R}^{J_s \times 64 \times 64}$$ 형태로 사용하며, heatmap은 2D joint coordinates 상의 gaussian blob으로 표현됩니다. 이떄 $$J_s$$는 30으로 설정되어 있는데, 여러 데이터셋에서 정의된 joint를 모두 합쳤을 때의 개수를 의미합니다. 3DCrowdNet은 정의되지 않았거나 confidence가 낮은 joint에 0을 곱해 무시하도록 했다고 합니다.

**cropped image** $$\mathbf{I} \in \mathbb{R}^{3 \times 256 \times 256}$$에서는 early-stage image feature of ResNet $$\mathbf{F} \in \mathbb{R}^{C \times 64 \times 64}$$를 얻습니다. 다음으로, $$\mathbf{F}$$와 $$\mathbf{H}^{2D}$$를 channel dimension을 따라 concat합니다. concat된 feature는 3x3 Convolution block을 통과하는데, feature의 HW는 유지하지만 채널은 $$\mathbf{F}$$의 채널 개수와 동일한 $$C$$로 바뀌게 됩니다. 다음으로는 ResNet을 통과하여 crowded scene-robust image feature $$\mathbf{F \prime} \in \mathbb{R}^{C \prime \times 8 \times 8}$$로 joint regressor에서 활용되며, $$C \prime = 2048$$입니다.


#### Joint-based regressor
Joint-based regressor는 $$\mathbf{F \prime}$$으로부터 3D joint coordinates $$\mathbf{P}^{3D} \in \mathbb{R}^{J_c \times 3}$$를 출력합니다. $$J_c=15$$인데, 여러 데이터셋에서 겹치는 joint들의 개수를 의미합니다. $$\mathbf{P}^{3D}$$의 (x, y) 좌표는 3D pixel space에서 정의되고, z 좌표는 root joint(배꼽점)에 상대적인 depth입니다.

그러면 Joint-based regressor에서 어떻게 $$\mathbf{P}^{3D}$$를 뽑는지를 알아보겠습니다. 우선 feature extractor에서 추출한 $$F \prime$$에서 $$J_cD$$ 차원의 2D feature map을 예측합니다. 다음 1X1 Convolution layer를 통해 3D heatmap $$\mathbf{H}^{3D}$$로 변환합니다. 이때 $$D=8$$인데 이산화된 크기의 depth를 의미합니다. (실수 값으로 나오는 depth를 이산 변수로 표현) 이 $$\mathbf{H}^{3D} \in \mathbb{R}^{J_c \times D \times 8 \times 8}$$로 soft-argmax operation을 하여 $$\mathbf{P}^{3D}$$를 계산합니다.

- soft argmax는 argmax의 미분 가능한 버전입니다.

다음으로, joint-based regressor는 사람의 global rotation $$\theta^g \in \mathbb{R}^3$$와 SMPL body rotation parameters $$\theta \in \mathbb{R}^{21 \times 3}$$, SMPL shape parameters $$\beta \in \mathbb{R}^{10}$$, projection을 위한 camera parameters $$k \in \mathbb{R}^3$$를 예측합니다. 우선, $$\mathbf{P}^{3D}$$의 (x, y) 픽셀 위치를 사용하여 $$\mathbf{F} \prime$$에서 joint별 image feature를 뽑습니다. 이때 (x, y) 픽셀 위치가 이산화된 값으로 나와있지 않기 때문에, bilinear interpolation을 적용했다고 합니다. 동일한 방식으로 $$\mathbf{P}^{3D}$$의 confidence를 $$\mathbf{H}^{3D}$$에서 추출합니다.

이후, joint별 image features와 $$\mathbf{P}^{3D}$$의 confidence를 concat하여 $$\mathbf{F}^{M} \in \mathbb{R}^{C \prime + 3 + 1}$$를 구합니다. 마지막으로, $$\mathbf{F}^M$$을 graph convolution network(GCN)로 처리하여 $$\theta^g$$를 예측합니다. $$\theta$$를 decode할 때 사용되는 latent code $$z$$, $$\beta$$, $$k$$도 GCN의 결과를 각기 다른 MLP를 통과하여 얻습니다.

## Loss function
3DCrowdNet의 loss function은 다음과 같이 정의됩니다.

![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/7895c607-1918-4563-9787-9a1f74794052)

$$L_{pose}$$는 예측된 $$\mathbf{P}^{3D}$$와 GT/pseudo GT 간의 L1 distance를 구합니다. $$L_{mesh}$$는 예측된 SMPL parameter의 loss입니다. 아래와 같이 정의됩니다.

![image](https://github.com/hannixxxoh/hannixxxoh/assets/91474981/0a49c599-8da1-407c-9bd1-e26f924a5d1c)

$$L_{param}$$은 예측된 $$\theta^g$$, $$\theta$$, $$\beta$$와 pseudo-GT parameter간의 L1 distance입니다. $$L_{pose \prime}$$은 예측된 mesh로부터 regress된 joint의 L1 distance loss입니다. 2D annotation이 데이터셋에 없는 경우, camera paramter $$k$$를 활용해서 projection하였습니다.